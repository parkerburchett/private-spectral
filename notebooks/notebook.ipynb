{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    raise ValueError(\"not on cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\n",
    "    \"data/0xFDC1BE05aD924e6Fc4Ab2c6443279fF7C0AB5544_training_data.parquet\"\n",
    ")\n",
    "test_df = df[-10_000:]\n",
    "train_df = df[:-10_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cols = [\n",
    "    \"wallet_age\",\n",
    "    \"incoming_tx_count\",\n",
    "    \"outgoing_tx_count\",\n",
    "    \"net_incoming_tx_count\",\n",
    "    \"total_gas_paid_eth\",\n",
    "    \"avg_gas_paid_per_tx_eth\",\n",
    "    \"risky_tx_count\",\n",
    "    \"risky_unique_contract_count\",\n",
    "    \"risky_first_tx_timestamp\",\n",
    "    \"risky_last_tx_timestamp\",\n",
    "    \"risky_first_last_tx_timestamp_diff\",\n",
    "    \"risky_sum_outgoing_amount_eth\",\n",
    "    \"outgoing_tx_sum_eth\",\n",
    "    \"incoming_tx_sum_eth\",\n",
    "    \"outgoing_tx_avg_eth\",\n",
    "    \"incoming_tx_avg_eth\",\n",
    "    \"max_eth_ever\",\n",
    "    \"min_eth_ever\",\n",
    "    \"total_balance_eth\",\n",
    "    \"risk_factor\",\n",
    "]\n",
    "\n",
    "target_cols = \"target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_df[training_cols].to_numpy(),\n",
    "    train_df[\"target\"].to_numpy(),\n",
    "    test_size=0.2,\n",
    "    random_state=24354325,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = sc.fit_transform(X_train)\n",
    "X_test_scaled = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PredictLiquidationsV1(nn.Module):\n",
    "    \"\"\"\n",
    "    The final layer should be a sigmoid, to get the probability of liquidation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_features, output_features, hidden_units):\n",
    "        super().__init__()\n",
    "        self.linear_layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=input_features, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_features),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_layer_stack(x)\n",
    "\n",
    "\n",
    "model_1 = PredictLiquidationsV1(\n",
    "    input_features=X_train.shape[1], output_features=1, hidden_units=82\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(params=model_1.parameters(), lr=0.001, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a dataset with 10,000 samples.\n",
    "X, y = make_circles(n_samples=10000, noise=0.05, random_state=26)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=26\n",
    ")\n",
    "\n",
    "# Visualize the data.\n",
    "fig, (train_ax, test_ax) = plt.subplots(\n",
    "    ncols=2, sharex=True, sharey=True, figsize=(10, 5)\n",
    ")\n",
    "train_ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.Spectral)\n",
    "train_ax.set_title(\"Training Data\")\n",
    "train_ax.set_xlabel(\"Feature #0\")\n",
    "train_ax.set_ylabel(\"Feature #1\")\n",
    "\n",
    "test_ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test)\n",
    "test_ax.set_xlabel(\"Feature #0\")\n",
    "test_ax.set_title(\"Testing data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
